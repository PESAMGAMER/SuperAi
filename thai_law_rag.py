import os
import json
import faiss
import torch
import numpy as np
import requests
import time
from typing import List
from tqdm import tqdm
from sentence_transformers import SentenceTransformer
from flask import Flask, request, render_template, jsonify

OLLAMA_URL = "http://localhost:11434/api/generate"
OLLAMA_MODEL = "llama3.2"  # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ä‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏ó‡∏µ‡πà‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡∏à‡∏£‡∏¥‡∏á
FOLDER_PATHS = ["data", "family"]  # ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÄ‡∏õ‡πá‡∏ô 3 folders
INDEX_PATH = "faiss.index"
TEXTS_PATH = "texts.json"

# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö GPU
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"üñ•Ô∏è ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏ö‡∏ô: {device.upper()}")

app = Flask(__name__)
embedder = SentenceTransformer("intfloat/multilingual-e5-base")
embedder.to(device)  # ‡∏¢‡πâ‡∏≤‡∏¢ model ‡πÑ‡∏õ‡∏ó‡∏µ‡πà GPU

# ‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£ global
texts = []
index = None

def load_laws(filepath: str) -> List[str]:
    with open(filepath, "r", encoding="utf-8") as f:
        data = json.load(f)
        
    results = []
    # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô list
    if isinstance(data, list):
        for item in data:
            try:
                # ‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà‡∏°‡∏µ law_name
                if 'law_name' in item and 'section_num' in item and 'section_content' in item:
                    results.append(f"{item['law_name']} ‡∏°‡∏≤‡∏ï‡∏£‡∏≤ {item['section_num']}:\n{item['section_content']}")
                # ‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÅ‡∏Ñ‡πà section_content
                elif 'section_content' in item:
                    results.append(item['section_content'])
            except Exception as e:
                print(f"  ‚ö†Ô∏è ‡∏Ç‡πâ‡∏≤‡∏° record ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå: {str(e)}")
                continue
    # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô dict ‡∏´‡∏£‡∏∑‡∏≠‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏≠‡∏∑‡πà‡∏ô
    else:
        print(f"  ‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö format ‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå: {filepath}")
        return []
        
    return results

def load_laws_from_folders(folder_paths: List[str]) -> List[str]:
    all_texts = []
    for folder_path in folder_paths:
        if not os.path.exists(folder_path):
            print(f"‚úó ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå {folder_path}")
            continue
            
        print(f"üìö ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡∏à‡∏≤‡∏Å‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå {folder_path}")
        
        # ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô files ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏Å‡πà‡∏≠‡∏ô
        json_files = [f for f in os.listdir(folder_path) if f.endswith('.json')]
        print(f"üìë ‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå .json ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {len(json_files)} ‡πÑ‡∏ü‡∏•‡πå")
        
        # ‡πÉ‡∏ä‡πâ tqdm ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏™‡∏î‡∏á progress bar
        for filename in tqdm(json_files, desc=f"‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏≤‡∏Å {folder_path}"):
            file_path = os.path.join(folder_path, filename)
            try:
                texts = load_laws(file_path)
                all_texts.extend(texts)
                print(f"  ‚úì ‡πÇ‡∏´‡∏•‡∏î {filename} ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à ({len(texts)} ‡∏°‡∏≤‡∏ï‡∏£‡∏≤)")
            except Exception as e:
                print(f"  ‚úó ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÇ‡∏´‡∏•‡∏î {filename}: {str(e)}")
                
        print(f"‚úÖ ‡πÇ‡∏´‡∏•‡∏î‡∏à‡∏≤‡∏Å {folder_path} ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô ‡∏£‡∏ß‡∏° {len(all_texts)} ‡∏°‡∏≤‡∏ï‡∏£‡∏≤\n")
        
    print(f"üìä ‡∏™‡∏£‡∏∏‡∏õ: ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {len(all_texts)} ‡∏°‡∏≤‡∏ï‡∏£‡∏≤ ‡∏à‡∏≤‡∏Å {len(folder_paths)} ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå")
    return all_texts

def embed_chunks(texts: List[str], batch_size: int = 16) -> np.ndarray:
    print("üîç ‡∏™‡∏£‡πâ‡∏≤‡∏á Embeddings...")
    return embedder.encode(texts, batch_size=batch_size, show_progress_bar=True,
                           convert_to_numpy=True, normalize_embeddings=True,
                           device=device)

def build_faiss_index(embeddings: np.ndarray) -> faiss.IndexFlatIP:
    print("üì¶ ‡∏™‡∏£‡πâ‡∏≤‡∏á FAISS Index...")
    if device == "cuda":
        # ‡πÉ‡∏ä‡πâ GPU ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö FAISS
        res = faiss.StandardGpuResources()
        index = faiss.IndexFlatIP(embeddings.shape[1])
        index = faiss.index_cpu_to_gpu(res, 0, index)
    else:
        index = faiss.IndexFlatIP(embeddings.shape[1])
    index.add(embeddings)
    return index

def search_similar_contexts(query: str, texts: List[str], index: faiss.IndexFlatIP, top_k: int = 5) -> List[str]:
    print("üß† ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•...")
    query_embedding = embedder.encode([query], convert_to_numpy=True, normalize_embeddings=True, device=device)
    scores, indices = index.search(query_embedding, top_k)

    threshold = 0.3  # ‡∏•‡∏î threshold ‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢
    filtered_results = [(texts[i], scores[0][idx]) for idx, i in enumerate(indices[0])
                        if scores[0][idx] > threshold]

    if not filtered_results:
        # fallback ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ú‡πà‡∏≤‡∏ô threshold
        print("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô threshold ‡πÉ‡∏ä‡πâ top-k ‡πÅ‡∏ó‡∏ô")
        filtered_results = [(texts[i], scores[0][idx]) for idx, i in enumerate(indices[0])]

    filtered_results.sort(key=lambda x: x[1], reverse=True)
    return [text for text, _ in filtered_results]

def generate_answer_ollama(context: str, query: str, max_tokens: int = 512) -> str:
    print("üîç ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á...")
    
    if context.strip():
        # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á
        print(f"üìñ ‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á {len(context.split('‡∏°‡∏≤‡∏ï‡∏£‡∏≤'))-1} ‡∏°‡∏≤‡∏ï‡∏£‡∏≤")
        prompt = f"""‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏î‡πâ‡∏≤‡∏ô‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏Ç‡πâ‡∏≠‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡πÉ‡∏´‡πâ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏á‡πà‡∏≤‡∏¢

‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á:
{context}

‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {query}

‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏≠‡∏ö‡πÇ‡∏î‡∏¢:
1. ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏î‡πâ‡∏ß‡∏¢‡∏†‡∏≤‡∏©‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏á‡πà‡∏≤‡∏¢
2. ‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏°‡∏≤‡∏ï‡∏£‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á
3. ‡∏¢‡∏Å‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)"""

    else:
        # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á
        print("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á")
        prompt = f"""‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏î‡πâ‡∏≤‡∏ô‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡∏ä‡πà‡∏ß‡∏¢‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ

‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {query}

‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏≠‡∏ö‡πÇ‡∏î‡∏¢:
1. ‡πÉ‡∏ä‡πâ‡∏†‡∏≤‡∏©‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏á‡πà‡∏≤‡∏¢
2. ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏ï‡∏≤‡∏°‡∏´‡∏•‡∏±‡∏Å‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ
3. ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÉ‡∏´‡πâ‡∏õ‡∏£‡∏∂‡∏Å‡∏©‡∏≤‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏à‡∏≤‡∏∞‡∏à‡∏á"""

    try:
        response = requests.post(OLLAMA_URL, json={
            "model": OLLAMA_MODEL,
            "prompt": prompt,
            "stream": False,
            "options": {
                "num_predict": max_tokens
            }
        })

        if response.status_code == 200:
            return response.json()["response"].strip()
        else:
            return f"‚ùå ‡∏ï‡∏¥‡∏î‡∏ï‡πà‡∏≠ Ollama ‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {response.status_code}"

    except Exception as e:
        return f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {str(e)}"

@app.route('/')
def home():
    return render_template('index.html')

@app.route('/health')
def health():
    return jsonify({"status": "ok"})

@app.route('/query', methods=['POST'])
def query():
    start_time = time.time()

    data = request.get_json()
    question = data.get('question', '').strip()

    if not question:
        return jsonify({'error': '‚ùå ‡πÇ‡∏õ‡∏£‡∏î‡∏£‡∏∞‡∏ö‡∏∏‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°'}), 400

    top_contexts = search_similar_contexts(question, texts, index)
    context = "\n\n".join(top_contexts)
    answer = generate_answer_ollama(context, question)

    return jsonify({
        'context': context,
        'answer': answer,
        'time_taken': f"{time.time() - start_time:.2f} ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ"
    })

if __name__ == "__main__":
    if os.path.exists(INDEX_PATH) and os.path.exists(TEXTS_PATH):
        print("üìÇ ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡πÅ‡∏Ñ‡∏ä...")
        index = faiss.read_index(INDEX_PATH)
        with open(TEXTS_PATH, "r", encoding="utf-8") as f:
            texts = json.load(f)
    else:
        texts = load_laws_from_folders(FOLDER_PATHS)
        if not texts:
            print("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå")
            exit(1)

        print(f"\nüìù ‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {len(texts)} ‡∏°‡∏≤‡∏ï‡∏£‡∏≤")
        embeddings = embed_chunks(texts)
        index = build_faiss_index(embeddings)

        faiss.write_index(index, INDEX_PATH)
        with open(TEXTS_PATH, "w", encoding="utf-8") as f:
            json.dump(texts, f, ensure_ascii=False)

    print("\nüéØ ‡∏£‡∏∞‡∏ö‡∏ö RAG (Ollama) ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô!\n")
    app.run(host='0.0.0.0', port=5000, debug=True)
